# Deep Learning Architectures

## Overview

This subfolder contains detailed implementations of various state-of-the-art deep learning architectures. Each model is implemented with comprehensive documentation and comments to aid users in understanding the complexities of each architecture. These models are designed for easy integration into larger projects or for standalone use in educational or research settings.

## Architectures Included

- **LeNet**: One of the earliest convolutional networks that helped propel the field of deep learning. (implementation pending)
- **AlexNet**: The CNN that notably won the ImageNet challenge by a large margin, sparking interest in deep learning for computer vision. (implementation pending)
- **VGG (Visual Geometry Group)**: Characterized by its simplicity, using only 3x3 convolutional layers stacked on top of each other in increasing depth. (implementation pending)
- **VAE (Variational Auto Encoder)**: Introduced a stochastic variational inference and training algorithm to approximate intractable posterior distribution, allowing reconstruction of a sample X from a lantent space Z. (work in progress üõ†Ô∏è)
- **Inception (GoogLeNet)**: Known for its network-in-network architecture with 1x1 convolutions to reduce dimensionality. (implementation pending)
- **ResNet (Residual Networks)**: Known for its deep network capabilities due to skip connections that help combat the vanishing gradient problem. (implementation pending)
- **U-Net**: Popular in medical image segmentation, featuring a symmetric expanding path that helps in precise localization. (implementation pending)
- **YOLO (You Only Look Once)**: For real-time object detection, this model processes images in a single evaluation and predicts bounding boxes and class labels. (implementation pending)
- **MobileNet**: Designed for use in mobile and edge devices, it uses depthwise separable convolutions to provide lightweight deep neural networks. (implementation pending)
- **EfficientNet**: Known for scaling up CNNs in a more structured and effective way to achieve better efficiency and accuracy. (implementation pending)
- **Transformers**: Originally designed for NLP tasks, they have since been adapted for use in various domains. (implementation pending)
- **BERT (Bidirectional Encoder Representations from Transformers)**: Revolutionizes the way embeddings are generated by considering the context of words from both directions. (implementation pending)
- **GPT (Generative Pre-trained Transformer)**: A model that uses transformers and has been fine-tuned for various NLP tasks. (implementation pending)

Each folder within this subfolder corresponds to a specific architecture and contains the following:
- Python implementation file(s)
- A brief tutorial on how to use the model
- Sample input and output
- Additional resources for further reading

## Usage

To use any of the models, navigate to the respective model's folder and follow the instructions in the README.md file located within that folder. Example usage is provided to demonstrate how to load data, train the model, and evaluate its performance.
